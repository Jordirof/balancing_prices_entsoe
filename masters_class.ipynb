{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing and day ahead electricity prices. \n",
    "**Can day ahead predict balancing prices?** \n",
    "\n",
    "**Purpose:** In this exercise we will attempt to answer this question taking data from ENTSOE fitting and testing some time series models.\n",
    "\n",
    "**Author:** Jordi Rof\n",
    "\n",
    "**Date:** 20/03/2024\n",
    "\n",
    "Note: You may also need to insall 'ipykernel' to make Jupyter work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import statsmodels.api as sm\n",
    "import statsmodels as sm\n",
    "import matplotlib as plt\n",
    "import re # for string substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and clean data\n",
    "In this exercise we will work with data obtained from ENTSOE.\n",
    "The data consist of 'day ahead' wholesale electricity prices and 'balancing' prices (aFRR).\n",
    "We see that we have day ahead prices and balancing prices, as expected.\n",
    "We have more than 35K observations that correspond to 2023 (every 15 minutes)\n",
    "We also see tha the order is not correct (pay attention to the hours at the bottom and the top!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('dayahead_imbalance.csv')\n",
    "# Explore dimensions\n",
    "print(data.columns)\n",
    "print(data.index)\n",
    "print(data.Datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. The text sting 'Datetime' does not have a standard format.\n",
    "2. The order is not correct.\n",
    "3. Some of the time features could be used as dummies that may contain important information of the behaviour of the vairables. Let's extract them.\n",
    "\"\"\"\n",
    "# Create time variables and order data correctly\n",
    "data['Year'] = data.Datetime.str.slice(0,4).astype(int)\n",
    "data['Month'] = data.Datetime.str.slice(5,7).astype(int)\n",
    "data['Day'] = data.Datetime.str.slice(8,10).astype(int)\n",
    "data['Hour'] = data.Datetime.str.slice(11,13).astype(int)\n",
    "data['Minute'] = data.Datetime.str.slice(14,16).astype(int)\n",
    "# Create a column with datetime format\n",
    "data['Time'] = data.Datetime.str.slice(0,16)\n",
    "data['Time'] = pd.to_datetime(data.Time, format='%Y-%m-%d %H:%M')\n",
    "# Sort by time\n",
    "data = data.sort_values('Time', ascending=True)\n",
    "# Drop the original Datetime\n",
    "data = data.drop('Datetime', axis = 1)\n",
    "# We will also use the oportunity for giving the data frame modeling friendly names\n",
    "data = data.rename(columns=lambda x: re.sub('_Day Ahead','_da',x))\n",
    "data.columns = data.columns.str.lower()\n",
    "# View newly created columns and a couple of extra variables\n",
    "print(data[['time','year','month','day','hour','minute','austria_da', 'long_austria']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. Obervation 0 is not present for balancing prices. We can simply remove it.\n",
    "2. Day ahead prices have NAs. This is because this variable is hourly, while the balancing prices are every 15 minutes. We can solve this with gap filling (carrying over the last observed value).\n",
    "3. The index is disorganized\n",
    "\"\"\"\n",
    "# Removing observation 0\n",
    "data = data.drop(0, axis = 0)\n",
    "# Filling missing values propagating the last valid value forward\n",
    "data = data.fillna(method='ffill')\n",
    "# Reset the index so it looks nicer\n",
    "data = data.reset_index()\n",
    "# Let's visualize some day ahead and balancing prices\n",
    "da_sample_list = ['austria_da', 'netherlands_da', 'belgium_da']\n",
    "bl_sample_list = ['long_austria', 'long_netherlands', 'long_belgium']\n",
    "print(data[da_sample_list].head(10))\n",
    "print(data[bl_sample_list].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring and understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by describing some of the day ahead and balancing prices\n",
    "print(data[da_sample_list].describe())\n",
    "print(data[bl_sample_list].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. Sufficient observations\n",
    "2. Similar means in day ahead and balancing\n",
    "3. Minimum and maximums are consistently higher in balancing than in day ahead\n",
    "\"\"\"\n",
    "# Let's continue with the correlation matrix (by country)\n",
    "print(data[['austria_da', 'long_austria', 'short_austria']].corr())\n",
    "print(data[['netherlands_da', 'long_netherlands', 'short_netherlands']].corr())\n",
    "print(data[['belgium_da', 'short_belgium', 'short_belgium']].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. We see that the correlation between day ahead and balancing is positive, but not strong. There may be a signal though.\n",
    "2. In the case of Austria and Belgium short and long are the same data (symmetry?)\n",
    "\"\"\"\n",
    "# We can try to visualize it with scatterplots and time serie splot\n",
    "data.plot.scatter(x = 'austria_da', y = 'long_austria', s = 100)\n",
    "data.plot.scatter(x = 'netherlands_da', y = 'long_netherlands', s = 100)\n",
    "data.plot.scatter(x = 'belgium_da', y = 'long_belgium', s = 100)\n",
    "data[['long_austria','austria_da']].plot()\n",
    "data[['long_netherlands','netherlands_da']].plot()\n",
    "data[['short_netherlands','netherlands_da']].plot()\n",
    "data[['long_belgium','belgium_da']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the plots shown above we can see that:\n",
    "1. The correlation between day ahead and balancing prices is visible.\n",
    "2. The volatility of balancing prices is evident.\n",
    "3. Means and standard deviations are quite stable over time.\n",
    "4. Volatility of balancing prices in the netherlands seems to be higher in H2, consistently reaching prices above 1500 eur.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling dataset\n",
    "In this section we add all the variables we need to the dataset.\n",
    "- Level differences\n",
    "- Lags\n",
    "- Intercept\n",
    "- Trends\n",
    "- Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's differentiate the data (not log differences as )\n",
    "data_diff1 = data.diff(periods=1)\n",
    "data_diff4 = data.diff(periods=4)\n",
    "data_diff1 = data_diff1.add_suffix(\"_diff1\",axis=1)\n",
    "data_diff4 = data_diff4.add_suffix(\"_diff4\",axis=1)\n",
    "# Let's add the laggs of the differences\n",
    "data_diff1_lag1 = data_diff1.shift(1)\n",
    "data_diff4_lag1 = data_diff4.shift(1)\n",
    "data_diff1_lag1 = data_diff1_lag1.add_suffix(\"_lag1\",axis=1)\n",
    "data_diff4_lag1 = data_diff4_lag1.add_suffix(\"_lag1\",axis=1)\n",
    "data_diff1_lag2 = data_diff1.shift(2)\n",
    "data_diff4_lag2 = data_diff4.shift(2)\n",
    "data_diff1_lag2 = data_diff1_lag2.add_suffix(\"_lag2\",axis=1)\n",
    "data_diff4_lag2 = data_diff4_lag2.add_suffix(\"_lag2\",axis=1)\n",
    "data_diff1_lag3 = data_diff1.shift(3)\n",
    "data_diff4_lag3 = data_diff4.shift(3)\n",
    "data_diff1_lag3 = data_diff1_lag3.add_suffix(\"_lag3\",axis=1)\n",
    "data_diff4_lag3 = data_diff4_lag3.add_suffix(\"_lag3\",axis=1)\n",
    "# Add dummies for some of the categorical variables\n",
    "dummies = pd.get_dummies(data.hour, dtype=float)\n",
    "# Concatenate the different dataframes created\n",
    "data = pd.concat([data,data_diff1,data_diff4,\n",
    "                  data_diff1_lag1,data_diff4_lag1,\n",
    "                  data_diff1_lag2,data_diff4_lag2,\n",
    "                  data_diff1_lag3,data_diff4_lag3,\n",
    "                  dummies],axis=1)\n",
    "# Add intercept and trens\n",
    "data['intercept'] = 1 # Add intercept for the regression\n",
    "data['lin_trend'] = data.index.values # Linear trend\n",
    "data['log_trend'] = np.log(data.index.values^2) # Add intercept for the regression\n",
    "# Drop all NAs so we do not have problems when fitting the models\n",
    "data = data.dropna()\n",
    "# Let's divide the series between the traning (or estimation) and test data. We will leave december out.\n",
    "data_train = data[(data.month < 12)].copy()\n",
    "data_test = data[(data.month == 12)].copy()\n",
    "# Check out the training and test data\n",
    "print(dummies.head(10))\n",
    "print(data_train[[\"time\",\"austria_da\",\"long_austria\",\"long_austria_diff1\"]].head(5))\n",
    "print(data_test[[\"time\",\"austria_da\",\"long_austria\",\"long_austria_diff1\"]].head(5))\n",
    "print(data_diff1[[\"long_austria_diff1\",\"long_belgium_diff1\"]].head(10))\n",
    "print(len(data.index)) # Check the amount of variables is still as expected\n",
    "# Visualize the trend\n",
    "data[['log_trend']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling\n",
    "In this section we start exploring some basic time series models.\n",
    "We will focus first on a country and see how well we can predict balancing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Austria ISF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run a quick stationarity test of the Austrian variables\n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html\n",
    "# The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. \n",
    "# If the pvalue is above a critical size, then we cannot reject that there is a unit root.\n",
    "print(sm.tsa.stattools.adfuller(data_train.austria_da_diff1.dropna()))\n",
    "print(sm.tsa.stattools.adfuller(data_train.long_austria_diff1.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. We are goood. It seems that our data is stationary.\n",
    "\"\"\"\n",
    "# Let's try to fit a simple AR(1) model\n",
    "model1a = sm.tsa.ar_model.AutoReg(data_train.long_austria_diff1.to_numpy(),1).fit()\n",
    "out = 'AIC: {0:0.3f}, HQIC: {1:0.3f}, BIC: {2:0.3f}'\n",
    "print('\\n Model 1a')\n",
    "print(out.format(model1a.aic, model1a.hqic, model1a.bic))\n",
    "print(model1a.params)\n",
    "print(model1a.pvalues)\n",
    "\n",
    "# Let's try the same model running the standard OLS command\n",
    "X =  data_train.long_austria_diff1\n",
    "Y1 = data_train[['intercept','long_austria_diff1_lag1']]\n",
    "model1b = sm.regression.linear_model.OLS(X, Y1).fit()\n",
    "print('\\n Model 1b')\n",
    "print(model1b.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. Model 1 and 2 are two different ways of fitting the same model.\n",
    "2. In model 1/2. The past values of the balancing market seem to have some explanatory power, negative coefficient.\n",
    "3. In model 1/2. The intercept is also strontly not significant.\n",
    "4. In model 1/2. The R2 is rather low.\n",
    "\"\"\"\n",
    "# Let's explore trends in the data\n",
    "Y2 = data_train[['intercept','long_austria_diff1_lag1','lin_trend','log_trend']]\n",
    "model2 = sm.regression.linear_model.OLS(X, Y2).fit()\n",
    "print('\\n Model 2')\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. None of the trends that we have tried improves the result at all.\n",
    "\"\"\"\n",
    "# Let's explore other lags\n",
    "Y3 = data_train[['intercept','long_austria_diff1_lag1','long_austria_diff1_lag2','long_austria_diff1_lag3']]\n",
    "model3 = sm.regression.linear_model.OLS(X, Y3).fit()\n",
    "print('\\n Model 3')\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In the printout shown above we can see that:\n",
    "1. Adding more lags to the model clearly improves it, but we still do not get a good model.\n",
    "    1.1. Better Adjusted R2 \n",
    "    1.2. Better lower AIC\n",
    "2. All the coefficients are still negative.\n",
    "3. I would pick model 4 over any of the previous.\n",
    "\"\"\"\n",
    "# Now we try to add the day ahead prices (diff4) to see if they improve our fcast\n",
    "Y4 = data_train[['intercept','long_austria_diff1_lag1','long_austria_diff1_lag2','long_austria_diff1_lag3','austria_da_diff4']]\n",
    "model4 = sm.regression.linear_model.OLS(X, Y4).fit()\n",
    "print('\\n Model 4')\n",
    "print(model4.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In the printout shown above we can see that:\n",
    "1. Day ahead prices appear to be significant. Therefore, they carry information.\n",
    "2. However, they do not improve any model selection metric (AIC, Adj.R2).\n",
    "3. All in all 3 and 4 are the best, no big difference.\n",
    "\"\"\"\n",
    "# It is worth cehcking what happens with only DA prices\n",
    "Y5 = data_train[['intercept','austria_da_diff4']]\n",
    "model5 = sm.regression.linear_model.OLS(X, Y5).fit()\n",
    "print('\\n Model 5')\n",
    "print(model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In the printout shown above we can see that:\n",
    "1. DA only do not carry as much signal as the lags, even though they are significant.\n",
    "2. All in all 3 and 4 are still the best.\n",
    "\"\"\"\n",
    "# Let's try another trick. Consider hours via dummies.\n",
    "#data_train.columns.values\n",
    "Y6 = data_train[['intercept','long_austria_diff1_lag1','long_austria_diff1_lag2','long_austria_diff1_lag3',7,8,12]]\n",
    "Y6\n",
    "model6 = sm.regression.linear_model.OLS(X, Y6).fit()\n",
    "print('\\n Model 6')\n",
    "print(model6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In the printout shown above we can see that:\n",
    "1. Hourly dummies suggest that prices have some tendency to increase / decrease in some particular hours.\n",
    "2. However, the effect is not strong, and it does not improve our model.\n",
    "2. All in all, not a big difference between 3,4,6.\n",
    "\"\"\"\n",
    "# Let's take a step back and collect the ISF results of the models that we created\n",
    "model_labels = ['model1','model2','model3','model4','model5','model6']\n",
    "isf_results = pd.DataFrame()\n",
    "#isf_results = isf_results.add_prefix('model_')\n",
    "isf_results['metric'] = ['R2','adjR2','AIC/1000','DF model']\n",
    "isf_results = isf_results.set_index('metric',drop=True)\n",
    "loop_data = [model1b,model2,model3,model4,model5,model6]\n",
    "isf_results['model1a'] = [np.nan,np.nan,model1a.aic/1000,np.nan]\n",
    "for i in range(len(model_labels)):\n",
    "    isf_results[model_labels[i]] = [loop_data[i].rsquared,\n",
    "                                    loop_data[i].rsquared_adj,\n",
    "                                    loop_data[i].aic/1000,\n",
    "                                    loop_data[i].df_model]\n",
    "# Print results\n",
    "print(isf_results.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the printout shown above we can see that:\n",
    "1. Models 3, 4 and 6 are the best so far. Lags of balancing prices bring most of the predicitive power.\n",
    "2. It is hard to add R2 with day ahead prices\n",
    "3. It is hard to add R2 with hourly dummies\n",
    "\"\"\"\n",
    "# Tip for finding the parameters. Save the attributes into a list.\n",
    "ls = []\n",
    "for attr in dir(model1b):\n",
    "    if not attr.startswith('_'):\n",
    "        ls.append(attr)\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Austria OOS\n",
    "Let's try to predict december."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the out of sample predicion of the models\n",
    "model1_oos_pred = model1b.predict(data_test[Y1.columns])\n",
    "model2_oos_pred = model2.predict(data_test[Y2.columns])\n",
    "model3_oos_pred = model3.predict(data_test[Y3.columns])\n",
    "model4_oos_pred = model4.predict(data_test[Y4.columns])\n",
    "model5_oos_pred = model5.predict(data_test[Y5.columns])\n",
    "model6_oos_pred = model6.predict(data_test[Y6.columns])\n",
    "\n",
    "# Let's first plot the results\n",
    "oos = pd.concat([data_test.long_austria_diff1,\n",
    "                 model1_oos_pred,model2_oos_pred,model3_oos_pred,\n",
    "                 model4_oos_pred,model5_oos_pred,model6_oos_pred], axis=1)\n",
    "oos.columns = ['long_austria_diff1'] + model_labels\n",
    "for i in range(len(model_labels)):\n",
    "    oos[['long_austria_diff1',model_labels[i]]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the plots shown above we can see that:\n",
    "1. The oos fit seems to be better in the models with high ISF.\n",
    "\"\"\"\n",
    "# Define some metrics for comfort\n",
    "def rmse(x,y):\n",
    "    op = ((x - y) ** 2).mean() ** .5\n",
    "    return op\n",
    "def mae(x,y):\n",
    "    op = (abs(x - y)).mean()\n",
    "    return op\n",
    "def emean(x,y):\n",
    "    op = (x - y).mean()\n",
    "    return op\n",
    "def emax(x,y):\n",
    "    op = abs(x - y).max()\n",
    "    return op\n",
    "# Now let's take a look at some informative metrics\n",
    "oos_results = pd.DataFrame()\n",
    "oos_results['metric'] = ['r','RMSE','MAE','error mean', 'max. error']\n",
    "oos_results = oos_results.set_index('metric',drop=True)\n",
    "loop_data = [oos['model1'],oos['model2'],oos['model3'],oos['model4'],oos['model5'],oos['model6']]\n",
    "for i in range(len(model_labels)):\n",
    "    oos_results[model_labels[i]] =[oos['long_austria_diff1'].corr(loop_data[i]),\n",
    "                            rmse(oos['long_austria_diff1'], loop_data[i]),\n",
    "                            mae(oos['long_austria_diff1'], loop_data[i]),\n",
    "                            emean(oos['long_austria_diff1'], loop_data[i]),\n",
    "                            emax(oos['long_austria_diff1'], loop_data[i])]\n",
    "# Print results\n",
    "print(oos_results.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### What have we learned?\n",
    "1. We have only tried Austria, and this not necessarily applies to all the countires.\n",
    "2. In Austria, day ahead prices appear to have some information about balancing prices, but they don't improve predictive power of the model a lot.\n",
    "3. The same can be said about hourly dummies.\n",
    "4. We are able to extrac much of the predictive power from lags.\n",
    "\n",
    "### Does it make sense that day ahead prices have a low predictive power?\n",
    "Yes. Balancing prices are a function of demand and supply mismatch in real time. In principle, there is no reason why day ahead prices should contain information about the mismatch. If the knowledge existed it would likely be used by traders.\n",
    "\n",
    "### What else could be done?\n",
    "1. With longer datasets, monthly effects could be explored.\n",
    "2. Calendar effects may have some explanatory power.\n",
    "3. Weather variables may have some explanatory power.\n",
    "4. Electricity system data such as frequency may have some explanatory power.\n",
    "5. Day ahead bid behaviour (no conensus amongst wind generators bid-offer?) could be a sensible gauge of uncertainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
